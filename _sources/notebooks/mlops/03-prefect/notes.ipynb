{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Orchestration and ML Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Status](https://img.shields.io/static/v1.svg?label=Status&message=Finished&color=brightgreen)\n",
    "[![Source](https://img.shields.io/static/v1.svg?label=GitHub&message=Source&color=181717&logo=GitHub)](https://github.com/particle1331/inefficient-networks/blob/master/docs/notebooks/mlops/03-prefect)\n",
    "[![Stars](https://img.shields.io/github/stars/particle1331/inefficient-networks?style=social)](https://github.com/particle1331/inefficient-networks)\n",
    "\n",
    "\n",
    "```text\n",
    "ùóîùòÅùòÅùóøùó∂ùóØùòÇùòÅùó∂ùóºùóª: Notes for Module 3 of the MLOps Zoomcamp (2022) by DataTalks.Club.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous module, we learned about experiment tracking and model registry.\n",
    "In particular, we discussed how to get a candidate model and promote it from staging to production.\n",
    "In this module, we learn how to automate this process, and having this scheduled with workflow orchestration &mdash; specifically with [**Prefect 2.0**](https://orion-docs.prefect.io/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prefect allows us to programatically author, schedule, and monitor workflows. One of the primary goals of Prefect is to minimize time spent on **negative engineering**, i.e. coding against all possible causes of failure. As there are endless ways that elements of a data pipeline can fail, you can potentially spend an endless amount of time on negative engineering. In practical terms, Prefect provides tools such as retries, concurrency, logging, a nice UI, tracking dependencies, a database, caching and serialization, parameterization of scheduled tasks, and more. As we shall see later, this can add observability to every step in the data pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **flow** in Prefect is simply a Python function. This consists of **tasks** which can be thought of as the atom of observability in Prefect. In practice, to create a flow, you simply convert functions that make it up into tasks. Consider the following example from the [*Getting Started with Prefect 2.0*](https://www.prefect.io/guide/blog/getting-started-prefect-2/#Makingourflowsbetterwithtasks) blog post. Here, we simulate getting data from an unreliable API, augmenting the fetched data, and writing the resulting data into a database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17:26:54.662 | INFO    | prefect.engine - Created flow run 'juicy-platypus' for flow 'pipeline'\n",
      "17:26:54.664 | INFO    | Flow run 'juicy-platypus' - Using task runner 'ConcurrentTaskRunner'\n",
      "17:26:54.671 | WARNING | Flow run 'juicy-platypus' - No default storage is configured on the server. Results from this flow run will be stored in a temporary directory in its runtime environment.\n",
      "17:26:54.708 | INFO    | Flow run 'juicy-platypus' - Created task run 'call_unreliable_api-48f93715-0' for task 'call_unreliable_api'\n",
      "17:26:54.729 | INFO    | Flow run 'juicy-platypus' - Created task run 'augment_data-505b3e0c-0' for task 'augment_data'\n",
      "17:26:54.741 | ERROR   | Task run 'call_unreliable_api-48f93715-0' - Encountered exception during execution:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/engine.py\", line 798, in orchestrate_task_run\n",
      "    result = await run_sync_in_worker_thread(task.fn, *args, **kwargs)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/utilities/asyncio.py\", line 54, in run_sync_in_worker_thread\n",
      "    return await anyio.to_thread.run_sync(call, cancellable=True)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_38107/3061218592.py\", line 10, in call_unreliable_api\n",
      "    raise Exception(\"Our unreliable service failed.\")\n",
      "Exception: Our unreliable service failed.\n",
      "17:26:54.766 | INFO    | Flow run 'juicy-platypus' - Created task run 'write_to_database-d58974ba-0' for task 'write_to_database'\n",
      "17:26:54.782 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\n",
      "17:26:54.804 | ERROR   | Task run 'call_unreliable_api-48f93715-0' - Encountered exception during execution:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/engine.py\", line 798, in orchestrate_task_run\n",
      "    result = await run_sync_in_worker_thread(task.fn, *args, **kwargs)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/utilities/asyncio.py\", line 54, in run_sync_in_worker_thread\n",
      "    return await anyio.to_thread.run_sync(call, cancellable=True)\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/to_thread.py\", line 31, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 937, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 867, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"/var/folders/jq/9vsvd9252_349lsng_5gc_jw0000gn/T/ipykernel_38107/3061218592.py\", line 10, in call_unreliable_api\n",
      "    raise Exception(\"Our unreliable service failed.\")\n",
      "Exception: Our unreliable service failed.\n",
      "17:26:54.822 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Received non-final state 'AwaitingRetry' when proposing final state 'Failed' and will attempt to run again...\n",
      "17:26:54.857 | INFO    | Task run 'call_unreliable_api-48f93715-0' - Finished in state Completed()\n",
      "17:26:54.885 | INFO    | Task run 'augment_data-505b3e0c-0' - Finished in state Completed()\n",
      "17:26:54.915 | INFO    | Task run 'write_to_database-d58974ba-0' - Finished in state Completed()\n",
      "17:26:54.925 | INFO    | Flow run 'juicy-platypus' - Finished in state Completed('All states completed.')\n",
      "Wrote {'data': 42, 'message': '0'} to database successfully!\n",
      "Completed(message='All states completed.', type=COMPLETED, result=[Completed(message=None, type=COMPLETED, result={'data': 42, 'message': '0'}, task_run_id=72365071-5085-4158-a391-eeddede5ca75), Completed(message=None, type=COMPLETED, result={'data': 42, 'message': '0'}, task_run_id=e7028889-7db0-435f-896c-03e6ff7bb733), Completed(message=None, type=COMPLETED, result='Success!', task_run_id=9ec022e6-39e8-41ea-98b2-b0a14df9eaff)], flow_run_id=be507fa9-289c-4ff1-a7eb-fd2aa29ab969)"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from prefect import flow, task \n",
    "\n",
    "\n",
    "@task(retries=3)\n",
    "def call_unreliable_api():\n",
    "    choices = [{\"data\": 42}, \"Failure\"]\n",
    "    res = random.choice(choices)\n",
    "    if res == \"Failure\":\n",
    "        raise Exception(\"Our unreliable service failed.\")\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "@task\n",
    "def augment_data(data: dict, msg: str):\n",
    "    data[\"message\"] = msg\n",
    "    return data\n",
    "\n",
    "@task\n",
    "def write_to_database(data: dict):\n",
    "    print(f\"Wrote {data} to database successfully!\")\n",
    "    return \"Success!\"\n",
    "\n",
    "@flow \n",
    "def pipeline(msg: str):\n",
    "    api_result = call_unreliable_api()\n",
    "    augmented_data = augment_data(data=api_result, msg=msg)\n",
    "    write_to_database(augmented_data)\n",
    "\n",
    "\n",
    "pipeline(0) # Augment data with zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefect Orion UI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this failed before pushing through. We can start the UI by calling `prefect orion start` in any directory (`.prefect` is saved in the system's root directory). This starts the Prefect Orion server in port 4200."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ prefect orion start\n",
    "Starting...\n",
    "\n",
    " ___ ___ ___ ___ ___ ___ _____    ___  ___ ___ ___  _  _\n",
    "| _ \\ _ \\ __| __| __/ __|_   _|  / _ \\| _ \\_ _/ _ \\| \\| |\n",
    "|  _/   / _|| _|| _| (__  | |   | (_) |   /| | (_) | .` |\n",
    "|_| |_|_\\___|_| |___\\___| |_|    \\___/|_|_\\___\\___/|_|\\_|\n",
    "\n",
    "Configure Prefect to communicate with the server with:\n",
    "\n",
    "    prefect config set PREFECT_API_URL=http://127.0.0.1:4200/api\n",
    "\n",
    "Check out the dashboard at http://127.0.0.1:4200\n",
    "\n",
    "\n",
    "\n",
    "INFO:     Started server process [20557]\n",
    "INFO:     Waiting for application startup.\n",
    "INFO:     Application startup complete.\n",
    "INFO:     Uvicorn running on http://127.0.0.1:4200 (Press CTRL+C to quit)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We navigate around the UI to find the `pipeline` flow and its most recent which, as we have seen in the logs, was able to complete its execution. Here we see that this flow started on `2022/06/10 11:05:51 PM` and ended on `2022/06/10 11:05:52 PM`. We also see the logs has the details of the exception when the API call failed. In the second tab, we can see the tasks that make up this flow. There is also the subflow tab which shows that we can call flows from a parent flow.\n",
    "\n",
    "```{figure} ../../../img/hello-world-2.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more interesting features of the dashboard is **Radar** on the right. This shows the dependence between tasks. Notice the linear dependence of the tasks, e.g. `write_to_database` depends on `augment_data` task but not on `call_unreliable_api`. Hovering on the tasks show the backward and forward data dependencies. Having tasks arranged in concentric circles allow for a heirarchy of dependence. Note that the runtime for each task is also conveniently displayed.\n",
    "\n",
    "```{figure} ../../../img/hello-world-1.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "Finally, let us look at a flow which failed to complete all its tasks. Here all calls to the API failed despite the retries. The radar plot nicely shows where the flow has failed. This is really useful, especially when we have a dozens task and multiple subflows happening in our data pipeline.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/hello-world-3.png\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "**Remark.** Note also that geometrically there is more space available to grow the dependence tree compared to top-down or left-right approaches due to nodes being farther apart as we move radially with a fixed angle, this also allows Radar to minimize edge crossing by combining radial and circumferential movement for the edges between task nodes. \n",
    "\n",
    "Furthermore, Radar dynamically updates as tasks complete (or fails). The mini-map, edge tracing, and node selection tools make workflow inspection doable even for highly complex graphs. See [*Introducing Radar*](https://www.prefect.io/guide/blog/introducing-radar/) for further reading.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow runs as flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will write Prefect flows for running modelling experiments as a flow in Prefect. Our idea is to define a `main` flow which consists of one subflow `preprocess_data` and two tasks which will execute MLflow runs. Note that the output of the `preprocess_data` subflow will be used by the two MLflow runs, so there will be some data dependency. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`utils.py`](https://github.com/particle1331/inefficient-networks/blob/85993956250601edeccf0d1bd5f192bb20873677/docs/notebooks/mlops/3-prefect/utils.py)\n",
    "```\n",
    "```python\n",
    "@task\n",
    "def load_training_dataframe(file_path, y_min=1, y_max=60):\n",
    "    \"\"\"Load data from disk and preprocess for training.\"\"\"\n",
    "    \n",
    "    # Load data from disk\n",
    "    data = pd.read_parquet(file_path)\n",
    "\n",
    "    # Create target column and filter outliers\n",
    "    data['duration'] = data.lpep_dropoff_datetime - data.lpep_pickup_datetime\n",
    "    data['duration'] = data.duration.dt.total_seconds() / 60\n",
    "    data = data[(data.duration >= y_min) & (data.duration <= y_max)]\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "@task\n",
    "def fit_preprocessor(train_data):\n",
    "    \"\"\"Fit and save preprocessing pipeline.\"\"\"\n",
    "\n",
    "    # Unpack passed data\n",
    "    y_train = train_data.duration.values\n",
    "    X_train = train_data.drop('duration', axis=1)    \n",
    "\n",
    "    # Initialize pipeline\n",
    "    categorical = ['PU_DO']\n",
    "    numerical = ['trip_distance']\n",
    "\n",
    "    preprocessor = make_pipeline(\n",
    "        PrepareFeatures(categorical, numerical),\n",
    "        DictVectorizer(),\n",
    "    )\n",
    "\n",
    "    # Fit only on train set\n",
    "    preprocessor.fit(X_train, y_train)\n",
    "    joblib.dump(preprocessor, artifacts / 'preprocessor.pkl')\n",
    "    \n",
    "    return preprocessor\n",
    "\n",
    "\n",
    "@task\n",
    "def create_model_features(preprocessor, train_data, valid_data):\n",
    "    \"\"\"Fit feature engineering pipeline. Transform training dataframes.\"\"\"\n",
    "\n",
    "    # Unpack passed data\n",
    "    y_train = train_data.duration.values\n",
    "    y_valid = valid_data.duration.values\n",
    "    X_train = train_data.drop('duration', axis=1)\n",
    "    X_valid = valid_data.drop('duration', axis=1)\n",
    "    \n",
    "    # Feature engineering\n",
    "    X_train = preprocessor.transform(X_train)\n",
    "    X_valid = preprocessor.transform(X_valid)\n",
    "\n",
    "    return X_train, y_train, X_valid, y_valid\n",
    "\n",
    "\n",
    "@flow\n",
    "def preprocess_data(train_data_path, valid_data_path):\n",
    "    \"\"\"Return feature and target arrays from paths. \n",
    "    Note: This just combines all the functions above in a single step.\"\"\"\n",
    "\n",
    "    train_data = load_training_dataframe(train_data_path)\n",
    "    valid_data = load_training_dataframe(valid_data_path)\n",
    "    preprocessor = fit_preprocessor(train_data)\n",
    "\n",
    "    # X_train, y_train, X_valid, y_valid\n",
    "    return create_model_features(preprocessor, train_data, valid_data).result()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will create the `main` flow described above. Here we are passing around a [`PrefectFuture`](https://orion-docs.prefect.io/api-ref/prefect/futures/) object instead of Python objects. Futures represent the execution of a task and allow retrieval of the task run's state. This so that Prefect is able to track data dependency between tasks &mdash; converting to Python objects, i.e. using `.result()`, breaks this lineage. Note that once a future has been passed into the function, then we can treat it as a usual Python object as the `task` wrapper has done work to unpack the future object into Python objects.\n",
    "\n",
    "For the `main` flow, we execute the following sequentially: running a subflow for preprocessing the datasets for modelling, training a linear regression baseline model, and training XGBoost models with different hyperparameters sampled using [TPE](https://optunity.readthedocs.io/en/latest/user/solvers/TPE.html). Sequential execution ensures that all resources are allocated to a single learning algorithm at each point in the flow run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`main.py`](https://github.com/particle1331/inefficient-networks/blob/85993956250601edeccf0d1bd5f192bb20873677/docs/notebooks/mlops/3-prefect/main.py)\n",
    "```\n",
    "```python\n",
    "def objective(params, xgb_train, y_train, xgb_valid, y_valid):\n",
    "    \"\"\"Compute validation RMSE (one trial = one run).\"\"\"\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        \n",
    "        model = xgb.train(\n",
    "            params=params,\n",
    "            dtrain=xgb_train,\n",
    "            num_boost_round=100,\n",
    "            evals=[(xgb_valid, 'validation')],\n",
    "            early_stopping_rounds=5,\n",
    "            verbose_eval=False\n",
    "        )\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "    return {'loss': rmse_valid, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "@task\n",
    "def xgboost_runs(num_runs, data):\n",
    "    \"\"\"Run TPE algorithm on search space to minimize objective.\"\"\"\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    xgb_train = xgb.DMatrix(X_train, label=y_train)\n",
    "    xgb_valid = xgb.DMatrix(X_valid, label=y_valid)\n",
    "\n",
    "    search_space = {\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),\n",
    "        'learning_rate': hp.loguniform('learning_rate', -3, 0),\n",
    "        'reg_alpha': hp.loguniform('reg_alpha', -5, -1),\n",
    "        'reg_lambda': hp.loguniform('reg_lambda', -6, -1),\n",
    "        'min_child_weight': hp.loguniform('min_child_weight', -1, 3),\n",
    "        'objective': 'reg:squarederror',\n",
    "        'seed': 42\n",
    "    }\n",
    "\n",
    "    best_result = fmin(\n",
    "        fn=partial(\n",
    "            objective, \n",
    "            xgb_train=xgb_train, y_train=y_train, \n",
    "            xgb_valid=xgb_valid, y_valid=y_valid,\n",
    "        ),\n",
    "        space=search_space,\n",
    "        algo=tpe.suggest,\n",
    "        max_evals=num_runs,\n",
    "        trials=Trials()\n",
    "    )\n",
    "\n",
    "\n",
    "@task\n",
    "def linreg_runs(data):\n",
    "    \"\"\"Run linear regression training.\"\"\"\n",
    "\n",
    "    X_train, y_train, X_valid, y_valid = data\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "\n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # MLflow logging\n",
    "        ...\n",
    "\n",
    "        \n",
    "@flow(task_runner=SequentialTaskRunner())\n",
    "def main(\n",
    "    train_data_path, \n",
    "    valid_data_path, \n",
    "    num_xgb_runs, \n",
    "    experiment_name,\n",
    "    tracking_uri,\n",
    "):\n",
    "    # Set and run experiment\n",
    "    mlflow.set_tracking_uri(tracking_uri)\n",
    "    mlflow.set_experiment(experiment_name)\n",
    "\n",
    "    data = preprocess_data(train_data_path, valid_data_path)\n",
    "    linreg_runs(data)\n",
    "    xgboost_runs(num_xgb_runs, data)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the dashboard, we can see a run of the `main` flow. As expected, this consists of 3 tasks that are executed sequentially as indicated in the timeline. \n",
    "\n",
    "```{figure} ../../../img/mlflow-runs-dashboard.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "Recall that the first task `preprocess-data` is a subflow. This has concurrent execution which we can see from overlapping lines in its timeline graph. This subflow consists of four tasks.\n",
    "\n",
    "```{figure} ../../../img/radar_preprocessing.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we check out the radar of the `main` flow, we see the following. Here in an earlier screenshot, we see that `xgboost_runs` is currently running for 1 minute and 14 seconds. Note that both MLflow runs depending on the preprocessing subflow can be seen by the presence of edges. We can go down on the radar for the `preprocess-data` subflow by clicking on the `4 task runs` button.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/radar_xgb.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "Here we see the radar plot. Hovering on each task shows its data dependence on other tasks. For each task, the forward and backward data dependency edges are shown by the Radar graph. For example, in the figure below we can see the dependencies for the task `load_training_dataframe` that loads the train dataset. This sends data to the preprocessor (for training) and to the final task (for transformation) which returns all processed data for modelling, so we can see two forward dependencies.\n",
    "\n",
    "```{figure} ../../../img/radar.png\n",
    "---\n",
    "---\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we deploy a workflow that puts the best model in an MLflow experiment to staging in the model registry. This can be useful for regularly staging candidate models models trained on new data. The staged models can then be further checked if it should be deployed into production. The code below is covered in [Experiment Tracking and Model Management](https://particle1331.github.io/inefficient-networks/notebooks/mlops/2-mlflow/2-mlflow.html#api-workflows)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model staging review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will run MLflow on localhost with SQLite. This can be easily adapted to a remote tracking server we only have to change `MLFLOW_TRACKING_URI`. Connecting to the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Experiment)\n",
      "    experiment_id=0\n",
      "    name='Default'\n",
      "    artifact_location='./mlruns/0'\n",
      "\n",
      "(Experiment)\n",
      "    experiment_id=1\n",
      "    name='nyc-taxi-experiment'\n",
      "    artifact_location='./mlruns/1'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "from mlflow.entities import ViewType\n",
    "\n",
    "MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "client = MlflowClient(tracking_uri=MLFLOW_TRACKING_URI)\n",
    "\n",
    "\n",
    "def print_experiment(experiment):\n",
    "    print(f\"(Experiment)\")\n",
    "    print(f\"    experiment_id={experiment.experiment_id}\")\n",
    "    print(f\"    name='{experiment.name}'\")\n",
    "    print(f\"    artifact_location='{experiment.artifact_location}'\")\n",
    "    print()\n",
    "\n",
    "for experiment in client.list_experiments():\n",
    "    print_experiment(experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the flow performs training a linear model and XGBoost models with different parameters. As sort of minimum requirements, we only consider those models with validation RMSE less than `6.5` and inference time less than `2e-5`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_id: 34d34a6ba92a4494bc1e6a7604b303f2   rmse_valid: 6.427   inference_time: 7.6936e-06\n",
      "run_id: fcf8eef67dd64db2848e0fecc1969914   rmse_valid: 6.481   inference_time: 4.2133e-06\n"
     ]
    }
   ],
   "source": [
    "candidates = client.search_runs(\n",
    "    experiment_ids=1,\n",
    "    filter_string='metrics.rmse_valid < 6.5 and metrics.inference_time < 20e-6',\n",
    "    run_view_type=ViewType.ACTIVE_ONLY,\n",
    "    max_results=5,\n",
    "    order_by=[\"metrics.rmse_valid ASC\"]\n",
    ")\n",
    "\n",
    "for run in candidates:\n",
    "    print(f\"run_id: {run.info.run_id}   rmse_valid: {run.data.metrics['rmse_valid']:.3f}   inference_time: {run.data.metrics['inference_time']:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having satisfied the operating requirements, we can take the model with lowest error as the new version and set it up for staging. Note that this flow can fail when there are no experiments in our tracker. But that is okay as it serves as notification for us to look into our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Successfully registered model 'NYCRideDurationModel'.\n",
      "2022/06/13 00:25:49 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: NYCRideDurationModel, version 1\n",
      "Created version '1' of model 'NYCRideDurationModel'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<ModelVersion: creation_timestamp=1655051149815, current_stage='Staging', description=None, last_updated_timestamp=1655051149828, name='NYCRideDurationModel', run_id='34d34a6ba92a4494bc1e6a7604b303f2', run_link=None, source='./mlruns/1/34d34a6ba92a4494bc1e6a7604b303f2/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=1>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_to_stage = candidates[0]\n",
    "\n",
    "registered_model = mlflow.register_model(\n",
    "    model_uri=f\"runs:/{model_to_stage.info.run_id}/model\", \n",
    "    name='NYCRideDurationModel'\n",
    ")\n",
    "\n",
    "client.transition_model_version_stage(\n",
    "    name='NYCRideDurationModel',\n",
    "    version=registered_model.version, \n",
    "    stage='Staging',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/mlflow-automatic-staging.png\n",
    "---\n",
    "---\n",
    "Staged model from code cells above.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow staging flow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks good, so we collect the above code cells along into a workflow which will create a new experiment, perform the experiment runs, and filter the best model for staging. We will then schedule this workflow to be run at fixed intervals using Prefect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`main.py`](https://github.com/particle1331/inefficient-networks/blob/06a1ecfea5456430538d13cdebbbe4c23dbbe93c/docs/notebooks/mlops/3-prefect/main.py)\n",
    "```\n",
    "```python\n",
    "@task\n",
    "def stage_model(tracking_uri, experiment_name):\n",
    "    \"\"\"Register and stage best model.\"\"\"\n",
    "\n",
    "    # Get best model from current experiment\n",
    "    client = MlflowClient(tracking_uri=tracking_uri)\n",
    "    candidates = client.search_runs(\n",
    "        experiment_ids=client.get_experiment_by_name(experiment_name).experiment_id,\n",
    "        filter_string='metrics.rmse_valid < 6.5 and metrics.inference_time < 20e-6',\n",
    "        run_view_type=ViewType.ACTIVE_ONLY,\n",
    "        max_results=5,\n",
    "        order_by=[\"metrics.rmse_valid ASC\"]\n",
    "    )\n",
    "\n",
    "    # Register and stage best model\n",
    "    best_model = candidates[0]\n",
    "    registered_model = mlflow.register_model(\n",
    "        model_uri=f\"runs:/{best_model.info.run_id}/model\", \n",
    "        name='NYCRideDurationModel'\n",
    "    )\n",
    "\n",
    "    client.transition_model_version_stage(\n",
    "        name='NYCRideDurationModel',\n",
    "        version=registered_model.version, \n",
    "        stage='Staging',\n",
    "    )\n",
    "\n",
    "    # Update description of staged model\n",
    "    client.update_model_version(\n",
    "        name='NYCRideDurationModel',\n",
    "        version=registered_model.version,\n",
    "        description=f\"[{datetime.now()}] The model version {registered_model.version} from experiment '{experiment_name}' was transitioned to Staging.\"\n",
    "    )\n",
    "\n",
    "...\n",
    "\n",
    "@flow(name='mlflow-staging', task_runner=SequentialTaskRunner())\n",
    "def mlflow_staging(\n",
    "    train_data_path, \n",
    "    valid_data_path, \n",
    "    num_xgb_runs=1\n",
    "):    \n",
    "    # Setup experiment\n",
    "    ctx = get_run_context()\n",
    "    MLFLOW_TRACKING_URI = \"sqlite:///mlflow.db\"\n",
    "    EXPERIMENT_NAME = f\"nyc-taxi-experiment-{ctx.flow_run.expected_start_time}\"\n",
    "\n",
    "    # Make experiment runs\n",
    "    main(\n",
    "        train_data_path=train_data_path, \n",
    "        valid_data_path=valid_data_path, \n",
    "        num_xgb_runs=num_xgb_runs, \n",
    "        experiment_name=EXPERIMENT_NAME,\n",
    "        tracking_uri=MLFLOW_TRACKING_URI,\n",
    "    )\n",
    "    \n",
    "    # Stage best model\n",
    "    stage_model(\n",
    "        tracking_uri=MLFLOW_TRACKING_URI, \n",
    "        experiment_name=EXPERIMENT_NAME\n",
    "    )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `mlflow_staging` creates a new experiment every time it runs with name based on the scheduled start time (obtained using the run context `ctx`). It's important that injecting the correct datetime parameter (for identifying the experiment) is delegated to the orchestrator instead of the executing machine. For example, the clock on the executing machine might be rouge, in a different timezone, or the job may be queued for some reason so that execution time is delayed. This is the case for the test run in the following figure.\n",
    "\n",
    "\n",
    "```{figure} ../../../img/flow-context.png\n",
    "---\n",
    "---\n",
    "Run context for a test flow that simply logs the scheduled time. For some reason, execution is delayed so that the execution time differs from the scheduled time. Also, the orchestrator time is in UTC (standard) while the other values are in UTC+8 (system time).\n",
    "```\n",
    "\n",
    "\n",
    "Later we see that we can specify a parameter dictionary for passing parameter values in the flow during deployment. So it is good practice to run a flow with a test parameters dict. Thus, we append the script with the following:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    parameters={\n",
    "        \"train_data_path\": './data/green_tripdata_2021-01.parquet',\n",
    "        \"valid_data_path\": './data/green_tripdata_2021-02.parquet',\n",
    "        \"num_xgb_runs\": 3,\n",
    "    }\n",
    "\n",
    "    mlflow_staging(**parameters)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00:28:34.569 | INFO    | prefect.engine - Created flow run 'flashy-dove' for flow 'mlflow-staging'\n",
      "00:28:34.569 | INFO    | Flow run 'flashy-dove' - Using task runner 'SequentialTaskRunner'\n",
      "00:28:34.623 | INFO    | Flow run 'flashy-dove' - Created subflow run 'uppish-earthworm' for flow 'main'\n",
      "2022/06/13 00:28:34 INFO mlflow.tracking.fluent: Experiment with name 'nyc-taxi-experiment-2022-06-13 00:28:34.236376' does not exist. Creating a new experiment.\n",
      "00:28:34.774 | INFO    | Flow run 'uppish-earthworm' - Created subflow run 'bizarre-trout' for flow 'preprocess-data'\n",
      "00:28:34.823 | INFO    | Flow run 'bizarre-trout' - Created task run 'load_training_dataframe-4335dacd-0' for task 'load_training_dataframe'\n",
      "00:28:34.851 | INFO    | Flow run 'bizarre-trout' - Created task run 'load_training_dataframe-4335dacd-1' for task 'load_training_dataframe'\n",
      "00:28:34.875 | INFO    | Flow run 'bizarre-trout' - Created task run 'fit_preprocessor-5181a278-0' for task 'fit_preprocessor'\n",
      "00:28:34.980 | INFO    | Flow run 'bizarre-trout' - Created task run 'create_model_features-3b628d84-0' for task 'create_model_features'\n",
      "00:28:35.834 | INFO    | Task run 'load_training_dataframe-4335dacd-1' - Finished in state Completed()\n",
      "00:28:36.159 | INFO    | Task run 'load_training_dataframe-4335dacd-0' - Finished in state Completed()\n",
      "00:28:37.454 | INFO    | Task run 'fit_preprocessor-5181a278-0' - Finished in state Completed()\n",
      "00:28:38.063 | INFO    | Task run 'create_model_features-3b628d84-0' - Finished in state Completed()\n",
      "00:28:38.143 | INFO    | Flow run 'bizarre-trout' - Finished in state Completed()\n",
      "00:28:38.156 | INFO    | Flow run 'uppish-earthworm' - Created task run 'linreg_runs-ec095b6c-0' for task 'linreg_runs'\n",
      "/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "00:28:41.477 | INFO    | Task run 'linreg_runs-ec095b6c-0' - Finished in state Completed()\n",
      "00:28:41.492 | INFO    | Flow run 'uppish-earthworm' - Created task run 'xgboost_runs-aa2fafb9-0' for task 'xgboost_runs'\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:21<00:00,  7.01s/trial, best loss: 6.457347294996046]\n",
      "00:29:02.638 | INFO    | Task run 'xgboost_runs-aa2fafb9-0' - Finished in state Completed()\n",
      "00:29:02.801 | INFO    | Flow run 'uppish-earthworm' - Finished in state Completed('All states completed.')\n",
      "00:29:02.826 | INFO    | Flow run 'flashy-dove' - Created task run 'stage_model-49154c66-0' for task 'stage_model'\n",
      "Registered model 'NYCRideDurationModel' already exists. Creating a new version of this model...\n",
      "2022/06/13 00:29:03 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: NYCRideDurationModel, version 3\n",
      "Created version '3' of model 'NYCRideDurationModel'.\n",
      "00:29:03.103 | INFO    | Task run 'stage_model-49154c66-0' - Finished in state Completed()\n",
      "00:29:03.256 | INFO    | Flow run 'flashy-dove' - Finished in state Completed('All states completed.')\n"
     ]
    }
   ],
   "source": [
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out `flashy-dove` in the Prefect Orion UI. Looks good. Here we see the `main` subflow for training and the `stage_model` task for staging the best model in the experiment that was just performed. This can all be checked in MLflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/flashy-dove.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LGTM, let's deploy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local storage setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a deployment in Prefect let us first setup a local storage for saving for persisting flow code for deployments, task results, and flow results. This is simple enough to do. Take note of the storage identifier as this will be used later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "$ prefect storage create\n",
    "\n",
    "Found the following storage types:\n",
    "0) Azure Blob Storage\n",
    "    Store data in an Azure blob storage container.\n",
    "1) File Storage\n",
    "    Store data as a file on local or remote file systems.\n",
    "2) Google Cloud Storage\n",
    "    Store data in a GCS bucket.\n",
    "3) Local Storage\n",
    "    Store data in a run's local file system.\n",
    "4) S3 Storage\n",
    "    Store data in an AWS S3 bucket.\n",
    "5) Temporary Local Storage\n",
    "    Store data in a temporary directory in a run's local file system.\n",
    "Select a storage type to create: 3\n",
    "You've selected Local Storage. It has 1 option(s).\n",
    "STORAGE PATH: ~/.prefect/local-storage\n",
    "Choose a new name for this storage configuration: local-storage\n",
    "Registered storage 'local-storage' with identifier\n",
    "'33133d27-a83b-468c-bb72-a9f19bd0d157'.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment specification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the deployment, we run the `mlflow-staging` flow every 5 minutes locally. We also set the parameters for each run. This adds a bit of flexibility. For now, the datasets are fixed, but it makes sense to run this on fresh data so that we actually get an updated model at each scheduled run. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{margin}\n",
    "[`deployments.py`](https://github.com/particle1331/inefficient-networks/blob/85993956250601edeccf0d1bd5f192bb20873677/docs/notebooks/mlops/3-prefect/deployments.py)\n",
    "```\n",
    "```python\n",
    "from prefect.deployments import DeploymentSpec\n",
    "from prefect.orion.schemas.schedules import IntervalSchedule\n",
    "from prefect.flow_runners import SubprocessFlowRunner\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "DeploymentSpec(\n",
    "    name=\"deploy-mlflow-staging\",\n",
    "    flow_name='mlflow-staging',\n",
    "    schedule=IntervalSchedule(interval=timedelta(minutes=1)),\n",
    "    flow_location=\"./main.py\",\n",
    "    flow_storage=\"33133d27-a83b-468c-bb72-a9f19bd0d157\", # local storage id\n",
    "    flow_runner=SubprocessFlowRunner(),\n",
    "    parameters={\n",
    "        \"train_data_path\": './data/green_tripdata_2021-01.parquet',\n",
    "        \"valid_data_path\": './data/green_tripdata_2021-02.parquet',\n",
    "        \"num_xgb_runs\": 10,\n",
    "    },\n",
    "    tags=[\"ml\"]\n",
    ")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying `SubprocessFlowRunner()` as flow runner, means that this flow is executed locally, e.g. not on Kubernetes or Docker containers. Here we specify the local storage identifier that we have just created above. Note that we specify the flow by name (`'mlflow-staging'`). This is why we defined `name='mlflow-staging'` in the decorator of `mlflow_staging`. To push our deployment to Prefect, we execute the following in the terminal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading deployment specifications from python script at \u001b[32m'deployments.py'\u001b[0m...\n",
      "/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/prefect/deployments.py:247: UserWarning: You have configured local storage, this deployment will only be usable from the current machine..\n",
      "  warnings.warn(\n",
      "Creating deployment \u001b[1;34m'deploy-mlflow-staging'\u001b[0m for flow \u001b[34m'mlflow-staging'\u001b[0m...\n",
      "Deploying flow script from \u001b[32m'/Users/particle1331/code/inefficient-networks/docs/n\u001b[0m\n",
      "\u001b[32motebooks/mlops/3-prefect/main.py'\u001b[0m using Local Storage...\n",
      "Created deployment \u001b[34m'mlflow-staging/\u001b[0m\u001b[1;34mdeploy-mlflow-staging'\u001b[0m.\n",
      "View your new deployment with: \n",
      "\n",
      "    prefect deployment inspect \u001b[34m'mlflow-staging/\u001b[0m\u001b[1;34mdeploy-mlflow-staging'\u001b[0m\n",
      "\u001b[32mCreated 1 deployments!\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!prefect deployment create deployments.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/deploy-mlflow-staging.png\n",
    "---\n",
    "---\n",
    "100 runs are now scheduled in Prefect.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding workers to deployed workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flows are now scheduled in Prefect. Notice that there are late runs. This is because we haven't attached any workers that will run these tasks. Unlike CI/CD platforms, all compute happens outside of Prefect that users will have to provide to run the scheduled workflows. So we create a **work queue** and fire up a **Prefect agent** to execute our deployment with our local compute. Note that the following setting up can also be done with the help of the UI.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[3m                                  Deployments                                  \u001b[0m\n",
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ\u001b[1m \u001b[0m\u001b[1mName                                \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mID                                  \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
      "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
      "‚îÇ\u001b[34m \u001b[0m\u001b[34mmlflow-staging/\u001b[0m\u001b[1;34mdeploy-mlflow-staging\u001b[0m\u001b[34m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36m271734ba-6e5e-4fa1-bf03-ab6801f7b44f\u001b[0m\u001b[36m \u001b[0m‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n"
     ]
    }
   ],
   "source": [
    "!prefect deployment ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;35mUUID\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'3f80fde3-4390-413f-be3a-3d8da1913163'\u001b[0m\u001b[1m)\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Copying the deployment ID above\n",
    "!prefect work-queue create \\\n",
    "    --deployment '271734ba-6e5e-4fa1-bf03-ab6801f7b44f' \\\n",
    "    --flow-runner subprocess \\\n",
    "    mlflow-deploy-runner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following shows the scheduled runs for this worker. This also confirms that we chose the correct flow runner and deployment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚îè‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î≥‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îì\n",
      "‚îÉ\u001b[1m \u001b[0m\u001b[1mScheduled Start‚Ä¶\u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mRun ID                \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mName  \u001b[0m\u001b[1m \u001b[0m‚îÉ\u001b[1m \u001b[0m\u001b[1mDeployment ID          \u001b[0m\u001b[1m \u001b[0m‚îÉ\n",
      "‚î°‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚ïá‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚î©\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:5‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36md381605d-9ceb-4314-bb‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mwoode‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:5‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36ma7e80fc1-99ac-40f4-bb‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mbrave‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:5‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36m93d3ad27-d7a6-48fa-bb‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mmeteo‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:5‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36ma7140abf-ffbd-4b13-b5‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mglori‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:5‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36m38227573-347a-4574-9c‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mdenim‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:5‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36m95572d03-9888-436b-9b‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32marche‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:4‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36m8195406c-62a1-44a3-95‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mthick‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:4‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36mbd41ec97-66e4-4061-be‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mvanil‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:4‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36me8427aaa-9a54-405c-95‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mnippy‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îÇ\u001b[33m \u001b[0m\u001b[33m2022-06-12 16:4‚Ä¶\u001b[0m\u001b[33m \u001b[0m‚îÇ\u001b[36m \u001b[0m\u001b[36m4a397219-dc7c-43a4-82‚Ä¶\u001b[0m\u001b[36m \u001b[0m‚îÇ\u001b[32m \u001b[0m\u001b[32mthick‚Ä¶\u001b[0m\u001b[32m \u001b[0m‚îÇ\u001b[34m \u001b[0m\u001b[34m271734ba-6e5e-4fa1-bf0‚Ä¶\u001b[0m\u001b[34m \u001b[0m‚îÇ\n",
      "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
      "\u001b[31m                            (**) denotes a late run                             \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Copying the worker ID above\n",
    "!prefect work-queue preview 3f80fde3-4390-413f-be3a-3d8da1913163"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the worker now, so it picks up scheduled runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting agent with ephemeral API...\n",
      "\n",
      "  ___ ___ ___ ___ ___ ___ _____     _   ___ ___ _  _ _____\n",
      " | _ \\ _ \\ __| __| __/ __|_   _|   /_\\ / __| __| \\| |_   _|\n",
      " |  _/   / _|| _|| _| (__  | |    / _ \\ (_ | _|| .` | | |\n",
      " |_| |_|_\\___|_| |___\\___| |_|   /_/ \\_\\___|___|_|\\_| |_|\n",
      "\n",
      "\n",
      "Agent started! Looking for work from queue \n",
      "'3f80fde3-4390-413f-be3a-3d8da1913163'...\n",
      "00:55:53.694 | INFO    | prefect.agent - Submitting flow run 'f01c4790-54bf-464b-b6c9-59b2104d4751'\n",
      "00:55:58.730 | INFO    | prefect.agent - Submitting flow run 'f01c4790-54bf-464b-b6c9-59b2104d4751'\n",
      "00:55:59.751 | INFO    | prefect.flow_runner.subprocess - Opening subprocess for flow run 'f01c4790-54bf-464b-b6c9-59b2104d4751'...\n",
      "00:55:59.761 | INFO    | prefect.agent - Completed submission of flow run 'f01c4790-54bf-464b-b6c9-59b2104d4751'\n",
      "00:56:02.765 | INFO    | Flow run 'gleaming-honeybee' - Using task runner 'SequentialTaskRunner'\n",
      "00:56:02.814 | INFO    | Flow run 'gleaming-honeybee' - Created subflow run 'wisteria-shrimp' for flow 'main'\n",
      "2022/06/13 00:56:02 INFO mlflow.tracking.fluent: Experiment with name 'nyc-taxi-experiment-2022-06-13 00:44:59.733397' does not exist. Creating a new experiment.\n",
      "00:56:02.924 | INFO    | Flow run 'wisteria-shrimp' - Created subflow run 'spotted-toucan' for flow 'preprocess-data'\n",
      "00:56:02.947 | INFO    | Flow run 'spotted-toucan' - Created task run 'load_training_dataframe-4335dacd-0' for task 'load_training_dataframe'\n",
      "00:56:02.961 | INFO    | Flow run 'spotted-toucan' - Created task run 'load_training_dataframe-4335dacd-1' for task 'load_training_dataframe'\n",
      "00:56:02.982 | INFO    | Flow run 'spotted-toucan' - Created task run 'fit_preprocessor-5181a278-0' for task 'fit_preprocessor'\n",
      "00:56:03.052 | INFO    | Flow run 'spotted-toucan' - Created task run 'create_model_features-3b628d84-0' for task 'create_model_features'\n",
      "00:56:03.920 | INFO    | Task run 'load_training_dataframe-4335dacd-0' - Finished in state Completed()\n",
      "00:56:04.309 | INFO    | Task run 'load_training_dataframe-4335dacd-1' - Finished in state Completed()\n",
      "00:56:04.886 | INFO    | Task run 'fit_preprocessor-5181a278-0' - Finished in state Completed()\n",
      "00:56:05.519 | INFO    | Task run 'create_model_features-3b628d84-0' - Finished in state Completed()\n",
      "00:56:05.624 | INFO    | Flow run 'spotted-toucan' - Finished in state Completed()\n",
      "00:56:05.664 | INFO    | Flow run 'wisteria-shrimp' - Created task run 'linreg_runs-f65b5121-0' for task 'linreg_runs'\n",
      "/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "00:56:09.216 | INFO    | Task run 'linreg_runs-f65b5121-0' - Finished in state Completed()\n",
      "00:56:09.226 | INFO    | Flow run 'wisteria-shrimp' - Created task run 'xgboost_runs-df0e77b4-0' for task 'xgboost_runs'\n",
      " 20%|‚ñà‚ñà        | 2/10 [00:25<01:37, 12.20s/trial, best loss: 6.454475362828172]00:56:54.176 | INFO    | prefect.agent - Submitting flow run '503b1a97-659b-403b-8364-d8b4b92a0811'\n",
      " 30%|‚ñà‚ñà‚ñà       | 3/10 [00:47<01:59, 17.00s/trial, best loss: 6.454475362828172]00:56:59.265 | INFO    | prefect.agent - Submitting flow run '503b1a97-659b-403b-8364-d8b4b92a0811'\n",
      "00:56:59.267 | INFO    | prefect.flow_runner.subprocess - Opening subprocess for flow run '503b1a97-659b-403b-8364-d8b4b92a0811'...\n",
      "00:56:59.279 | INFO    | prefect.agent - Completed submission of flow run '503b1a97-659b-403b-8364-d8b4b92a0811'\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:51<01:11, 11.87s/trial, best loss: 6.454475362828172]00:57:03.606 | INFO    | Flow run 'therapeutic-dogfish' - Using task runner 'SequentialTaskRunner'\n",
      "00:57:03.687 | INFO    | Flow run 'therapeutic-dogfish' - Created subflow run 'beryl-seriema' for flow 'main'\n",
      "00:57:03.982 | INFO    | Flow run 'beryl-seriema' - Created subflow run 'abiding-spider' for flow 'preprocess-data'\n",
      "00:57:04.043 | INFO    | Flow run 'abiding-spider' - Created task run 'load_training_dataframe-4335dacd-0' for task 'load_training_dataframe'\n",
      "00:57:04.098 | INFO    | Flow run 'abiding-spider' - Created task run 'load_training_dataframe-4335dacd-1' for task 'load_training_dataframe'\n",
      "00:57:04.351 | INFO    | Flow run 'abiding-spider' - Created task run 'fit_preprocessor-5181a278-0' for task 'fit_preprocessor'\n",
      "00:57:04.491 | INFO    | Flow run 'abiding-spider' - Created task run 'create_model_features-3b628d84-0' for task 'create_model_features'\n",
      "00:57:05.902 | INFO    | Task run 'load_training_dataframe-4335dacd-1' - Finished in state Completed()\n",
      "00:57:06.411 | INFO    | Task run 'load_training_dataframe-4335dacd-0' - Finished in state Completed()\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 5/10 [00:57<00:47,  9.55s/trial, best loss: 6.454475362828172]00:57:07.390 | INFO    | Task run 'fit_preprocessor-5181a278-0' - Finished in state Completed()\n",
      "00:57:08.423 | INFO    | Task run 'create_model_features-3b628d84-0' - Finished in state Completed()\n",
      "00:57:08.627 | INFO    | Flow run 'abiding-spider' - Finished in state Completed()\n",
      "00:57:08.658 | INFO    | Flow run 'beryl-seriema' - Created task run 'linreg_runs-f65b5121-0' for task 'linreg_runs'\n",
      "/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "00:57:12.653 | INFO    | Task run 'linreg_runs-f65b5121-0' - Finished in state Completed()\n",
      "00:57:12.671 | INFO    | Flow run 'beryl-seriema' - Created task run 'xgboost_runs-df0e77b4-0' for task 'xgboost_runs'\n",
      " 30%|‚ñà‚ñà‚ñà       | 3/10 [00:41<01:52, 16.09s/trial, best loss: 6.392997003417292]]00:57:54.731 | INFO    | prefect.agent - Submitting flow run '2740040e-6031-4e51-a30a-641dd4d9d361'\n",
      "00:57:59.784 | INFO    | prefect.agent - Submitting flow run '2740040e-6031-4e51-a30a-641dd4d9d361'\n",
      "00:57:59.802 | INFO    | prefect.flow_runner.subprocess - Opening subprocess for flow run '2740040e-6031-4e51-a30a-641dd4d9d361'...\n",
      "00:57:59.814 | INFO    | prefect.agent - Completed submission of flow run '2740040e-6031-4e51-a30a-641dd4d9d361'\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [00:52<01:22, 13.74s/trial, best loss: 6.392997003417292]00:58:06.948 | INFO    | Flow run 'charcoal-buffalo' - Using task runner 'SequentialTaskRunner'\n",
      "00:58:07.066 | INFO    | Flow run 'charcoal-buffalo' - Created subflow run 'daffy-stoat' for flow 'main'\n",
      "00:58:07.313 | INFO    | Flow run 'daffy-stoat' - Created subflow run 'mutant-mouflon' for flow 'preprocess-data'\n",
      "00:58:07.364 | INFO    | Flow run 'mutant-mouflon' - Created task run 'load_training_dataframe-4335dacd-0' for task 'load_training_dataframe'\n",
      "00:58:07.397 | INFO    | Flow run 'mutant-mouflon' - Created task run 'load_training_dataframe-4335dacd-1' for task 'load_training_dataframe'\n",
      "00:58:07.420 | INFO    | Flow run 'mutant-mouflon' - Created task run 'fit_preprocessor-5181a278-0' for task 'fit_preprocessor'\n",
      "00:58:07.457 | INFO    | Flow run 'mutant-mouflon' - Created task run 'create_model_features-3b628d84-0' for task 'create_model_features'\n",
      "00:58:09.260 | INFO    | Task run 'load_training_dataframe-4335dacd-1' - Finished in state Completed()\n",
      "00:58:09.784 | INFO    | Task run 'load_training_dataframe-4335dacd-0' - Finished in state Completed()\n",
      "00:58:11.697 | INFO    | Task run 'fit_preprocessor-5181a278-0' - Finished in state Completed()\n",
      "00:58:12.775 | INFO    | Task run 'create_model_features-3b628d84-0' - Finished in state Completed()\n",
      "00:58:12.888 | INFO    | Flow run 'mutant-mouflon' - Finished in state Completed()\n",
      "00:58:12.929 | INFO    | Flow run 'daffy-stoat' - Created task run 'linreg_runs-f65b5121-0' for task 'linreg_runs'\n",
      "/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "00:58:17.515 | INFO    | Task run 'linreg_runs-f65b5121-0' - Finished in state Completed()\n",
      "00:58:17.540 | INFO    | Flow run 'daffy-stoat' - Created task run 'xgboost_runs-df0e77b4-0' for task 'xgboost_runs'\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10/10 [02:36<00:00, 15.69s/trial, best loss: 6.4070710286201145]\n",
      "00:58:46.764 | INFO    | Task run 'xgboost_runs-df0e77b4-0' - Finished in state Completed()\n",
      "00:58:47.246 | INFO    | Flow run 'wisteria-shrimp' - Finished in state Completed('All states completed.')\n",
      "00:58:47.290 | INFO    | Flow run 'gleaming-honeybee' - Created task run 'stage_model-18da4b31-0' for task 'stage_model'\n",
      "Registered model 'NYCRideDurationModel' already exists. Creating a new version of this model...\n",
      "2022/06/13 00:58:47 INFO mlflow.tracking._model_registry.client: Waiting up to 300 seconds for model version to finish creation.                     Model name: NYCRideDurationModel, version 4\n",
      "Created version '4' of model 'NYCRideDurationModel'.\n",
      "00:58:47.599 | INFO    | Task run 'stage_model-18da4b31-0' - Finished in state Completed()\n",
      "00:58:47.783 | INFO    | Flow run 'gleaming-honeybee' - Finished in state Completed('All states completed.')\n",
      "00:58:49.267 | INFO    | prefect.flow_runner.subprocess - Subprocess for flow run 'f01c4790-54bf-464b-b6c9-59b2104d4751' exited cleanly.\n",
      "00:58:50.835 | INFO    | prefect.agent - Submitting flow run '53519e88-4739-48b4-af24-1e44bcecf186'\n",
      "00:58:55.971 | INFO    | prefect.agent - Submitting flow run '53519e88-4739-48b4-af24-1e44bcecf186'\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 6/10 [01:46<01:27, 21.95s/trial, best loss: 6.392997003417292]00:58:59.982 | INFO    | prefect.flow_runner.subprocess - Opening subprocess for flow run '53519e88-4739-48b4-af24-1e44bcecf186'...\n",
      "00:58:59.992 | INFO    | prefect.agent - Completed submission of flow run '53519e88-4739-48b4-af24-1e44bcecf186'\n",
      " 20%|‚ñà‚ñà        | 2/10 [00:45<03:05, 23.13s/trial, best loss: 6.4342406106230365]00:59:07.929 | INFO    | Flow run 'light-lion' - Using task runner 'SequentialTaskRunner'\n",
      "00:59:08.062 | INFO    | Flow run 'light-lion' - Created subflow run 'crystal-barracuda' for flow 'main'\n",
      "00:59:08.272 | INFO    | Flow run 'crystal-barracuda' - Created subflow run 'economic-partridge' for flow 'preprocess-data'\n",
      "00:59:08.319 | INFO    | Flow run 'economic-partridge' - Created task run 'load_training_dataframe-4335dacd-0' for task 'load_training_dataframe'\n",
      "00:59:08.351 | INFO    | Flow run 'economic-partridge' - Created task run 'load_training_dataframe-4335dacd-1' for task 'load_training_dataframe'\n",
      "00:59:08.398 | INFO    | Flow run 'economic-partridge' - Created task run 'fit_preprocessor-5181a278-0' for task 'fit_preprocessor'\n",
      "00:59:08.581 | INFO    | Flow run 'economic-partridge' - Created task run 'create_model_features-3b628d84-0' for task 'create_model_features'\n",
      "00:59:10.284 | INFO    | Task run 'load_training_dataframe-4335dacd-1' - Finished in state Completed()\n",
      "00:59:10.903 | INFO    | Task run 'load_training_dataframe-4335dacd-0' - Finished in state Completed()\n",
      "00:59:12.521 | INFO    | Task run 'fit_preprocessor-5181a278-0' - Finished in state Completed()\n",
      "00:59:13.755 | INFO    | Task run 'create_model_features-3b628d84-0' - Finished in state Completed()\n",
      "00:59:13.915 | INFO    | Flow run 'economic-partridge' - Finished in state Completed()\n",
      "00:59:13.956 | INFO    | Flow run 'crystal-barracuda' - Created task run 'linreg_runs-f65b5121-0' for task 'linreg_runs'\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 7/10 [02:05<01:02, 20.72s/trial, best loss: 6.337603948759769]/Users/particle1331/miniforge3/envs/prefect/lib/python3.9/site-packages/_distutils_hack/__init__.py:30: UserWarning: Setuptools is replacing distutils.\n",
      "  warnings.warn(\"Setuptools is replacing distutils.\")\n",
      "00:59:19.908 | INFO    | Task run 'linreg_runs-f65b5121-0' - Finished in state Completed()\n",
      "00:59:19.939 | INFO    | Flow run 'crystal-barracuda' - Created task run 'xgboost_runs-df0e77b4-0' for task 'xgboost_runs'\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 8/10 [02:35<00:47, 23.80s/trial, best loss: 6.337603948759769] 00:59:52.747 | INFO    | prefect.agent - Submitting flow run 'dae0ee38-992c-434b-8c7d-23c660aa6af1'\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 4/10 [01:38<02:24, 24.08s/trial, best loss: 6.395515995851409]00:59:57.840 | INFO    | prefect.agent - Submitting flow run 'dae0ee38-992c-434b-8c7d-23c660aa6af1'\n",
      "00:59:59.958 | INFO    | prefect.flow_runner.subprocess - Opening subprocess for flow run 'dae0ee38-992c-434b-8c7d-23c660aa6af1'...\n",
      "00:59:59.971 | INFO    | prefect.agent - Completed submission of flow run 'dae0ee38-992c-434b-8c7d-23c660aa6af1'\n",
      " 10%|‚ñà         | 1/10 [00:42<06:23, 42.63s/trial, best loss: 6.408626467921648]^C\n"
     ]
    }
   ],
   "source": [
    "# Copying the worker ID above\n",
    "!prefect agent start 3f80fde3-4390-413f-be3a-3d8da1913163"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We stop the run since this will take a while. But we already see that it has completed one flow, while some flows are still running, some are late, and many are scheduled. The late flows are because we had 1 minute deployments and it took a while for us to create a worker. Another nice thing to notice with the worker works concurrently on late flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{figure} ../../../img/deployment-1.png\n",
    "---\n",
    "---\n",
    "```\n",
    "\n",
    "```{figure} ../../../img/deployment-2.png\n",
    "---\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the log on `00:58:47.290` we see that the completed flow `gleaming-honeybee` created Version 4 of the model `NYCRideDurationModel`. Indeed, we have this version staged in the model registry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<ModelVersion: creation_timestamp=1655053127536, current_stage='Staging', description=('[2022-06-13 00:58:47.571025] The model version 4 from experiment '\n",
       "  \"'nyc-taxi-experiment-2022-06-13 00:44:59.733397' was transitioned to \"\n",
       "  'Staging.'), last_updated_timestamp=1655053127571, name='NYCRideDurationModel', run_id='cf5964fcf6dd47189ba6a5707360155f', run_link=None, source='./mlruns/8/cf5964fcf6dd47189ba6a5707360155f/artifacts/model', status='READY', status_message=None, tags={}, user_id=None, version=4>]"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.get_latest_versions(name='NYCRideDurationModel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('ml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a55a0d1272a360f93e747858d443ec26da69f69eac36db3e567a961ca624a861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
